{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install neomodel torch scikit-learn transformers numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neomodel import (StructuredNode, StringProperty, RelationshipTo, db, config, UniqueIdProperty)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from neomodel import db\n",
    "\n",
    "\n",
    "config.DATABASE_URL = \"bolt://neo4j:<PASSWORD>@localhost:7687\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**tokens).last_hidden_state[:, 0, :]\n",
    "    return embeddings.squeeze(0).cpu().numpy()\n",
    "\n",
    "def store_embeddings_in_neo4j():\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Paper)\n",
    "    WHERE p.embedding IS NULL\n",
    "    RETURN p.title AS title, p.abstract AS abstract, ID(p) AS id\n",
    "    \"\"\"\n",
    "    results, _ = db.cypher_query(query)\n",
    "    \n",
    "    for row in results:\n",
    "        abstract = row[1]\n",
    "        paper_id = row[2]\n",
    "        \n",
    "        embedding = encode_text(abstract).tolist()  \n",
    "\n",
    "        update_query = \"\"\"\n",
    "        MATCH (p:Paper)\n",
    "        WHERE ID(p) = $id\n",
    "        SET p.embedding = $embedding\n",
    "        \"\"\"\n",
    "        db.cypher_query(update_query, {'id': paper_id, 'embedding': embedding})\n",
    "    \n",
    "    print(f\"Stored embeddings for {len(results)} papers without embeddings.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_embeddings_in_neo4j()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def fetch_filtered_papers(query):\n",
    "    cypher_query = \"\"\"\n",
    "    MATCH (p:Paper)\n",
    "    WHERE p.abstract CONTAINS $query OR p.title CONTAINS $query\n",
    "    RETURN p.title AS title, p.embedding AS embedding, ID(p) AS id\n",
    "    \"\"\"\n",
    "    results, _ = db.cypher_query(cypher_query, {'query': query})\n",
    "    return results\n",
    "\n",
    "def compute_similarity(query_embedding, paper_embedding):\n",
    "    return 1 - cosine(query_embedding, paper_embedding)\n",
    "\n",
    "\n",
    "def semantic_search(query, top_n=5, similarity_threshold=0.2):\n",
    "    query_embedding = encode_text(query)\n",
    "\n",
    "    filtered_papers = fetch_filtered_papers(query)\n",
    "\n",
    "    if not filtered_papers:\n",
    "        return \"No papers found that match the query.\"\n",
    "\n",
    "    results_with_similarity = []\n",
    "    for paper in filtered_papers:\n",
    "        if paper[1] is not None:\n",
    "            title = paper[0]\n",
    "            paper_embedding = torch.tensor(paper[1])  \n",
    "            similarity = compute_similarity(query_embedding, paper_embedding)\n",
    "\n",
    "            if similarity >= similarity_threshold:\n",
    "                results_with_similarity.append((title, similarity))\n",
    "\n",
    "    if not results_with_similarity:\n",
    "        return \"No relevant papers found with sufficient similarity.\"\n",
    "\n",
    "    results_with_similarity.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results_with_similarity[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"cosmological models\"\n",
    "\n",
    "top_papers = semantic_search(query)\n",
    "\n",
    "if isinstance(top_papers, str):\n",
    "    print(top_papers)  \n",
    "else:\n",
    "    for title, score in top_papers:\n",
    "        print(f\"Title: {title}, Similarity: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
